{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 PREPARING TO MODEL THE DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUPERVISED VERSUS UNSUPERVISED METHODS**\n",
    "\n",
    "- In unsupervised methods, no target variable is identified as such.\n",
    "- We do not mean to imply that unsupervised methods require no human involvement. To the contrary, effective cluster analysis and association rule mining both require substantial human judgment and skill."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STATISTICAL METHODOLOGY AND DATA MINING METHODOLOGY**\n",
    "- In statistical methodology, the data analyst has an a <u>priori hypothesis</u> in mind.\n",
    "Data mining procedures usually do not have an a priori hypothesis, instead\n",
    "freely trolling through the data for actionable results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSS-VALIDATION**\n",
    "- Data mining can become data dredging, whereby the analyst uncovers <u>phantom spurious results</u>, due to random variation rather than\n",
    "real effects. This is accomplished through cross-validation.\n",
    "- the algorithm needs to guard against <u>memorizing</u> the training set and blindly applying all patterns found in the training set to the future data.\n",
    "- Estimates of <u>model performance</u> for future, unseen data can then be computed by observing various evaluative measures applied to the test data set.\n",
    "- The data analyst must ensure that the training and test data sets are indeed <u>independent</u> by validating the partition.\n",
    "- In <u>k-fold cross-validation</u>, the original data are partitioned into k independent and similar subsets. The model is then built using the data from k – 1 subsets, using the kth subset as the test set. This is done iteratively until we have k different models. The results from the k models are then combined using averaging or voting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERFITTING**\n",
    "- There is an eternal tension in model building between <u>model complexity</u> (resulting in high accuracy on the training set) and <u>generalizability</u> to the test and validation sets. \n",
    "<p style=\"width: 30%; border: 2px solid; text-align: center;\">\n",
    "<img src=\"./img/L06.04.00.png\"> <br>\n",
    "<p/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIAS–VARIANCE TRADE-OFF**\n",
    "- Even though the <u>high complexity model</u> has a <u>low bias</u> (in terms of the error rate on the training set), it has a <u>high variance</u>; and even though the <u>low complexity model</u> has a <u>high bias</u>, it has a <u>low variance</u>. This is what is known as the <u>bias–variance trade-off</u>. \n",
    "- The bias–variance trade-off is another way of describing the <u>overfitting/underfitting dilemma</u>. The goal is to construct a model in which neither the bias nor the variance is too high, but usually, minimizing one tends to increase the other.\n",
    "- A common method of evaluating how accurate model estimation is proceeding for a continuous target variable is to use the <u>mean-squared error (MSE)</u>. It combines both bias and variance. The MSE is a function of the <u>estimation error (SSE)</u> and the <u>model complexity (e.g., degrees of freedom)</u>. \n",
    "- $MSE = variance + bias^2$\n",
    "\n",
    "<p style=\"width: 70%; border: 2px solid; text-align: center;\">\n",
    "<img src=\"./img/L06.05.00.png\" style=\"width: 30%;\">\n",
    "<img src=\"./img/L06.05.01.png\" style=\"width: 30%;\">\n",
    "<img src=\"./img/L06.05.02.png\" style=\"width: 30%;\">\n",
    "<p/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BALANCING THE TRAINING DATA SET**\n",
    "- For classification models in which <u>one of the target variable classes</u> has much lower relative frequency than the other classes, balancing is recommended. \n",
    "- <u>Resample</u> some records with much low frequency or <u>set aside</u> some records with high frequency.\n",
    "- The <u>test data set</u> should <u>never be balanced</u>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ESTABLISHING BASELINE PERFORMANCE**\n",
    "\n",
    "- Without comparison to a baseline, a client cannot determine whether our results are any good.\n",
    "\n",
    "- The type of baseline one should use depends on the way the results are reported.\n",
    "    - Suppose our data mining model resulted in a predicted churn rate of 9.99%. This represents only a 14.49% – 9.99% = 4.5% absolute decrease in the churn rate, but a 4.5% ∕ 14.49% = 31% relative decrease in the churn rate. <u>The analyst should make it clear for the client which comparison method is being used</u>.\n",
    "\n",
    "    - A more challenging yardstick against which to calibrate your model is to use existing research or results in the field. For example, suppose the algorithm your analytics company currently uses succeeds in identifying 90% of all fraudulent online transactions. Then your company will probably expect your new data mining model to outperform this 90% baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISES**\n",
    "2,3,4,6,8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
