{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 k-NEAREST NEIGHBOR ALGORITHM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFICATION TASK**\n",
    "\n",
    "Perhaps the most common data mining task is that of classification.\n",
    "\n",
    "* Banking: determining whether a mortgage application is a good or bad credit risk, or whether a particular credit card transaction is fraudulent\n",
    "* Education: placing a new student into a particular track with regard to special needs\n",
    "* Medicine: diagnosing whether a particular disease is present\n",
    "* Law: determining whether a will was written by the actual person deceased or fraudulently by someone else\n",
    "* Homeland security: identifying whether or not certain financial or personal behavior indicates a possible terrorist threat\n",
    "\n",
    "<img src=\"./img/L07.01.01.png\" style=\"width: 50%; box-shadow: 1px 1px 1px 1px grey;\"> <br>\n",
    "<!-- <p style=\"width: 50%; border: 1px solid; text-align: center;\">\n",
    "<img src=\"./img/L07.01.01.png\"> <br>\n",
    "<p/> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-NEAREST NEIGHBOR ALGORITHM**\n",
    "\n",
    "The first algorithm we shall investigate is the k-nearest neighbor algorithm, which is <u>most often used for classification</u>, although it can also be used for estimation and prediction. k-Nearest neighbor is an example of <u>instance-based learning</u>, in which the training data set is stored, so that a classification for a new unclassified record may be found simply by comparing it to the most similar records in the training set.\n",
    "\n",
    "\n",
    "- Interested in classifying the type of drug a patient should be prescribed, based on certain patient characteristics, such as the age of the patient and the patient’s sodium/potassium (Na/K) ratio.\n",
    "    - The particular drug prescribed is symbolized by the shade of the points. <u>Light gray</u> points indicate <u>drug Y</u>; <u>medium gray</u> points indicate <u>drug A or X</u>; <u>dark gray</u> points indicate <u>drug B or C</u>.\n",
    "    - Now suppose that we have a <u>new patient (1)</u> record, without a drug classification. This patient is <u>40 years old</u> and has <u>a Na/K ratio of 29</u>. Since all patients are prescribed drug Y, we would thereby classify new patient 1 as drug Y. \n",
    "    - New patient 2, who is <u>17 years old</u> with <u>a Na/K ratio of 12.5</u>. Suppose we let <u>k = 1 for our k-nearest neighbor algorithm</u>, so that new patient 2 would be classified according to whichever single (one) observation it was closest to. In this case, <u>new patient 2</u> would be classified for <u>drugs B and C (dark gray)</u>\n",
    "    - The classification of the <u>k = 2, voting would not help</u>. However, if we let <u>k = 3</u> for the algorithm, so that new patient 2 would be classified based on the three points closest to it. <u>Since two of the three closest points are medium gray</u>, a classification based on voting would therefore choose <u>drugs A and X (medium gray)</u> as the classification for new patient 2. Note that the classification assigned for new patient 2 differed based on which value we chose for k.\n",
    "    - Finally, consider new patient 3, who is 47 years old and has a Na/K ratio of 13.5. Voting would not help for k = 3 in this case either, since the three nearest neighbors to new patient 3 are of three different classifications.\n",
    "- Some of the issues involved in building a classifier using the k-nearest neighbor algorithm:\n",
    "    - How many neighbors should we consider? That is, what is k?\n",
    "    - How do we measure distance?\n",
    "    - How do we combine the information from more than one observation?\n",
    "    - Should all points be weighted equally, or should some points have more influence than others?\n",
    "\n",
    "<img src=\"./img/L07.01.02.png\" style=\"width: 30%; box-shadow: 1px 1px 1px 1px gray;\">\n",
    "<img src=\"./img/L07.01.03.png\" style=\"width: 20%; box-shadow: 1px 1px 1px 1px gray;\">\n",
    "<img src=\"./img/L07.01.04.png\" style=\"width: 20%; box-shadow: 1px 1px 1px 1px gray;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTANCE FUNCTION**\n",
    "\n",
    "- Data analysts define distance metrics to <u>measure similarity</u>. \n",
    "1. d(x, y) ≥ 0, and d(x, y) = 0 if and only if x = y\n",
    "2. d(x, y) = d(y, x)\n",
    "3. d(x, z) ≤ d(x, y) + d(y, z)\n",
    "\n",
    "\n",
    "$$d_{\\text{Euclidean}}(x, y) = \\sqrt{\\sum_{i = 1,...,\\text{Number of Attributes}}{(x_i − y_i)^2}}$$\n",
    "\n",
    "\n",
    "<img src=\"./img/L07.03.01.png\" style=\"width: 30%; box-shadow: 1px 1px 1px 1px gray;\">\n",
    "<img src=\"./img/L07.03.02.png\" style=\"width: 30%;\"> \n",
    "<br>\n",
    "<br><img src=\"./img/L07.03.03.png\" style=\"width: 30%;\"><br>\n",
    "\n",
    "For categorical variables, the Euclidean distance metric is not appropriate.\n",
    "<br><img src=\"./img/L07.03.04.png\" style=\"width: 20%;\"><br>\n",
    "\n",
    "Therefore, perhaps, when mixing categorical and continuous variables, the min-max normalization may be preferred.\n",
    "<br> <img src=\"./img/L07.03.05.png\" style=\"width: 50%;\"> <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMBINATION FUNCTION**\n",
    "\n",
    "**Simple Unweighted Voting**\n",
    "For k = 3, a classification based on simple voting would choose drugs A and X (medium gray) as the classification for new patient 2, <u>since two of the three closest points are medium gray</u>. The classification would then be made for drugs A and X, <u>with confidence 66.67%</u>, where the confidence level represents the count of records, with the winning classification divided by k.<br>\n",
    "On the other hand, <u>in Figure 7.3, for k = 3, simple voting would fail</u> to choose a clear winner since each of the three categories receives one vote. There would be a tie among the three classifications represented by the records in Figure 7.3, and a tie may not be a preferred result.\n",
    "\n",
    "**Weighted Voting**\n",
    "The analyst may choose to apply weighted voting, where closer neighbors have a larger voice in the classification decision than do more distant neighbors. Weighted voting also makes it much less likely for ties to arise.\n",
    "\n",
    "<br> <img src=\"./img/L07.04.01.png\" style=\"width: 30%;\"> <br>\n",
    "<br> <img src=\"./img/L07.04.02.png\" style=\"width: 30%;\"> <br>\n",
    "<br> <img src=\"./img/L07.04.03.png\" style=\"width: 30%;\"> <br>\n",
    "<br> <img src=\"./img/L07.04.04.png\" style=\"width: 40%;\"> <br>\n",
    "\n",
    "When the <u>distance is zero</u>, the inverse would be <u>undefined</u>. In this case the algorithm should choose the <u>majority</u> classification of <u>all records whose distance is zero</u> from the new record. <br>\n",
    "Consider for a moment that once we begin weighting the records, there is <u>no theoretical reason why we could not increase k</u> arbitrarily so that all existing records are included in the weighting. However, this runs up against the practical consideration of <u>very slow computation times</u> for calculating the weights of all of the records every time a new record needs to be classified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUANTIFYING ATTRIBUTE RELEVANCE: STRETCHING THE AXES**\n",
    "\n",
    "This can be accomplished using a cross-validation approach or one based on domain expert knowledge. First, note that the problem of determining which fields are more or less important is equivalent to finding a coefficient zj by which to multiply the jth axis, with larger values of zj associated with more important variable axes. This process is therefore termed stretching the axes.\n",
    "\n",
    "For example, suppose that either through cross-validation or expert knowledge, the Na/K ratio was determined to be three times as important as age for drug classification. Then we would have $z_{Na/K} = 3$ and $z_{age} = 1$. For the example above, the new distances of records A, B, and C from the new record would be as follows:\n",
    "\n",
    "<br> <img src=\"./img/L07.05.01.png\" style=\"width: 40%;\"> <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATABASE CONSIDERATIONS**\n",
    "\n",
    "It is especially important that <u>rare classifications be represented sufficiently</u>, so that the algorithm does not <u>only predict common classifications</u>.\n",
    "The data set would need <u>to be balanced</u>, with a sufficiently <u>large percentage of the less common</u> classifications. \n",
    "<u>One method</u> to perform balancing is to <u>reduce the proportion of records with more common classifications</u>.\n",
    "Main memory may fill up, and access to auxiliary storage <u>is slow</u>. Therefore, if the database is to be used <u>for k-nearest neighbor</u> methods only, it may be helpful to <u>retain only</u> those data points that are <u>near a classification “boundary.”</u>\n",
    "For example, in Figure 7.1, all records with Na/K ratio value greater than, say, 19 could be omitted from the database <u>without loss of classification accuracy</u>, since all records in this region are classified as light gray\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-NEAREST NEIGHBOR ALGORITHM FOR ESTIMATION AND PREDICTION**\n",
    "\n",
    "The k-nearest neighbor algorithm may be used for estimation and prediction as well as for continuousvalued target variables. This is called locally weighted\n",
    "averaging.\n",
    "\n",
    "Assume that we are using the $z_{Na/K}$ = three-axis stretching to reflect the greater importance of the Na/K ratio.\n",
    "<br> <img src=\"./img/L07.07.01.png\" style=\"width: 40%;\"> <br>\n",
    "<br> <img src=\"./img/L07.07.02.png\" style=\"width: 10%;\"> <br>\n",
    "\n",
    "where $w_i = 1∕d(\\text{new}, x_i)^2$ for existing records $x_1, x_2, … , x_k$. Thus, in this example, the estimated systolic blood pressure reading for the new record would be\n",
    "\n",
    "<br> <img src=\"./img/L07.07.03.png\" style=\"width: 40%;\"> <br>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHOOSING k**\n",
    "\n",
    "It is possible to allow the data itself to help resolve this problem, by following a <u>cross-validation</u> procedure similar to the earlier method for finding the optimal values $z_1, z_2, … , z_m$ for axis stretching. Here we would try various values of k with different randomly selected training sets and <u> choose the value of k that minimizes the classification or estimation error </u>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISES**\n",
    "3, 4, 5, 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Python Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>income</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>other</td>\n",
       "      <td>28060.70</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>other</td>\n",
       "      <td>28009.34</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>other</td>\n",
       "      <td>27614.60</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>other</td>\n",
       "      <td>27287.18</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>other</td>\n",
       "      <td>26954.06</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>51</td>\n",
       "      <td>married</td>\n",
       "      <td>46810.12</td>\n",
       "      <td>good risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>55</td>\n",
       "      <td>married</td>\n",
       "      <td>45709.78</td>\n",
       "      <td>good risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>51</td>\n",
       "      <td>married</td>\n",
       "      <td>44896.42</td>\n",
       "      <td>good risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>54</td>\n",
       "      <td>married</td>\n",
       "      <td>44301.52</td>\n",
       "      <td>good risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>60</td>\n",
       "      <td>married</td>\n",
       "      <td>54096.00</td>\n",
       "      <td>good risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age marital_status    income       risk\n",
       "0     34          other  28060.70   bad loss\n",
       "1     37          other  28009.34   bad loss\n",
       "2     29          other  27614.60   bad loss\n",
       "3     33          other  27287.18   bad loss\n",
       "4     39          other  26954.06   bad loss\n",
       "..   ...            ...       ...        ...\n",
       "241   51        married  46810.12  good risk\n",
       "242   55        married  45709.78  good risk\n",
       "243   51        married  44896.42  good risk\n",
       "244   54        married  44301.52  good risk\n",
       "245   60        married  54096.00  good risk\n",
       "\n",
       "[246 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ClassifyRisk = pd.read_csv(\"./data_sets/DKD2e_data_sets/ClassifyRisk\")\n",
    "df = ClassifyRisk[['age', 'marital_status', 'income', 'risk']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real risk for 0 is bad loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>income</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>other</td>\n",
       "      <td>27287.18</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>33</td>\n",
       "      <td>other</td>\n",
       "      <td>27134.72</td>\n",
       "      <td>bad loss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age marital_status    income      risk\n",
       "3    33          other  27287.18  bad loss\n",
       "57   33          other  27134.72  bad loss"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if df[feature_name].dtype == 'object': continue\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def dist(a,b):\n",
    "    res = 0\n",
    "    if a['marital_status']!=b['marital_status']: res += 1\n",
    "    res += (a['age']-b['age'])**2\n",
    "    res += (a['income']-b['income'])**2\n",
    "    res = np.sqrt(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "df_norm = normalize(df)\n",
    "dist_list = []\n",
    "n = df.shape[0]\n",
    "j = 0\n",
    "k = 2\n",
    "for i in range(n):\n",
    "    dist_list.append([i,dist(df_norm.iloc[j],df_norm.iloc[i])])\n",
    "\n",
    "print(f\"Real risk for {j} is {df['risk'].loc[j]}\")\n",
    "\n",
    "dist_list.sort(key = lambda x: x[1])\n",
    "knearest = [ i[0] for i in dist_list[1:k+1]]\n",
    "\n",
    "df.loc[knearest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
